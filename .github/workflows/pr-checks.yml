# PR Checks Workflow
# Story: 0-8 GitHub Actions Workflow Setup (base)
# Story: 0-10 Automated Test Execution on PR (enhanced)
# Story: 0-14 Test Database Provisioning (tenant schemas + RLS)
# Story: 0-16 CI/CD Test Pipeline Integration (parallel, reporting, coverage)
# Story: 0-17 Test Reporting Dashboard (Allure Server integration)
# AC: 1 - Parallel test execution (unit, integration, E2E)
# AC: 2 - Test results uploaded as artifacts (JUnit XML, coverage)
# AC: 3 - Test summary PR comment with pass/fail counts and coverage
# AC: 4 - Failed tests show stack traces and error logs
# AC: 5 - Flaky test retry (3x unit, 2x E2E)
# AC: 6 - Execution time tracked (target <10 min)
# AC: 7 - Node.js matrix (18, 20)
# AC: 8 - Database migrations + seed before integration tests
# AC: 9 - Coverage threshold 80% enforced
# AC: 10 - Artifact retention 30 days

name: PR Checks

on:
  pull_request:
    branches: [main]

permissions:
  contents: read
  pull-requests: write
  checks: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===================================================================
  # Lint (ESLint + Ruff)
  # ===================================================================
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install Python dependencies
        run: pip install ruff

      - name: Run Ruff
        run: ruff check .

  # ===================================================================
  # Format Check (Prettier + Black)
  # ===================================================================
  format-check:
    name: Format Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Check Prettier formatting
        run: npx prettier --check .

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install Black
        run: pip install black

      - name: Check Black formatting
        run: black --check .

  # ===================================================================
  # Type Check (TypeScript + mypy)
  # ===================================================================
  type-check:
    name: Type Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run TypeScript type check
        run: npx tsc --noEmit

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      - name: Run mypy
        run: mypy .

  # ===================================================================
  # Unit Tests (AC1, AC3, AC5, AC7, AC9)
  # Matrix: Node.js 18 + 20 | Coverage collection | Retry 3x
  # ===================================================================
  unit-tests:
    name: Unit Tests (Node ${{ matrix.node-version }})
    needs: [lint, format-check, type-check]
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        node-version: ["18", "20"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests with coverage
        run: npm test -- --coverage --maxWorkers=4
        env:
          CI: "true"

      - name: Upload coverage to Codecov (AC9)
        if: matrix.node-version == '20'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/lcov.info
          flags: unit-tests
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-node-${{ matrix.node-version }}
          path: |
            coverage/
            test-results/
            allure-results/
          retention-days: 30

      - name: Report unit test results (AC2, AC6)
        if: always() && matrix.node-version == '20'
        uses: dorny/test-reporter@v1
        with:
          name: Unit Test Results (Node ${{ matrix.node-version }})
          path: test-results/junit.xml
          reporter: jest-junit
          fail-on-error: true

  # ===================================================================
  # Integration Tests (AC1, AC5, AC10)
  # PostgreSQL + Redis service containers | Database migrations
  # ===================================================================
  integration-tests:
    name: Integration Tests
    needs: [lint, format-check, type-check]
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: qualisys_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U test_user"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    env:
      DATABASE_URL: postgresql://test_user:test_password@localhost:5432/qualisys_test
      TEST_DATABASE_URL: postgresql://test_user:test_password@localhost:5432/qualisys_test
      REDIS_URL: redis://localhost:6379
      NODE_ENV: test

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Initialize test database (tenant schemas + RLS)
        run: |
          PGPASSWORD=test_password psql \
            -h localhost -U test_user -d qualisys_test \
            -f infrastructure/scripts/init-test-db.sql

      - name: Run database migrations
        run: npm run db:migrate:test

      - name: Seed test data
        run: npm run db:seed:test
        env:
          TEST_DATABASE_URL: postgresql://test_user:test_password@localhost:5432/qualisys_test

      - name: Run integration tests
        run: npm run test:integration
        env:
          CI: "true"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            test-results/
            allure-results/
          retention-days: 30

      - name: Report integration test results (AC2, AC6)
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Integration Test Results
          path: test-results/junit.xml
          reporter: jest-junit
          fail-on-error: true

  # ===================================================================
  # E2E Tests — Critical Paths Only (AC1, AC5)
  # Playwright with chromium | Retry 2x | Timeout 5 min
  # ===================================================================
  e2e-tests:
    name: E2E Tests (Critical)
    needs: [lint, format-check, type-check]
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Run critical path E2E tests
        run: npx playwright test --config=e2e/playwright.config.ts --project=chromium-critical
        timeout-minutes: 5
        env:
          CI: "true"

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: |
            e2e/playwright-report/
            e2e/test-results/
            e2e/allure-results/
          retention-days: 30

      - name: Report E2E test results (AC2, AC6)
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: E2E Test Results (Critical)
          path: e2e/test-results/e2e-results.xml
          reporter: java-junit
          fail-on-error: true

  # ===================================================================
  # PR Summary Comment (AC2, AC3, AC4, AC6)
  # Downloads artifacts, parses JUnit XML + coverage, posts PR comment
  # ===================================================================
  test-summary:
    name: Test Summary
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-artifacts
        continue-on-error: true

      - name: Parse results and post PR comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // --- Job-level results ---
            const jobResults = {
              unit: '${{ needs.unit-tests.result }}',
              integration: '${{ needs.integration-tests.result }}',
              e2e: '${{ needs.e2e-tests.result }}',
            };

            const statusIcon = (s) => s === 'success' ? '✅' : s === 'skipped' ? '⏭️' : '❌';
            const allPassed = Object.values(jobResults).every(r => r === 'success');

            // --- Parse JUnit XML for test counts (AC3) ---
            function parseJUnit(dir) {
              const counts = { tests: 0, passed: 0, failed: 0, skipped: 0, errors: 0, time: 0 };
              try {
                const walk = (d) => {
                  for (const entry of fs.readdirSync(d, { withFileTypes: true })) {
                    const full = path.join(d, entry.name);
                    if (entry.isDirectory()) walk(full);
                    else if (entry.name.endsWith('.xml')) {
                      const xml = fs.readFileSync(full, 'utf8');
                      const t = xml.match(/tests="(\d+)"/);
                      const f = xml.match(/failures="(\d+)"/);
                      const e = xml.match(/errors="(\d+)"/);
                      const s = xml.match(/skipped="(\d+)"/);
                      const tm = xml.match(/time="([\d.]+)"/);
                      if (t) counts.tests += parseInt(t[1]);
                      if (f) counts.failed += parseInt(f[1]);
                      if (e) counts.errors += parseInt(e[1]);
                      if (s) counts.skipped += parseInt(s[1]);
                      if (tm) counts.time += parseFloat(tm[1]);
                    }
                  }
                };
                if (fs.existsSync(dir)) walk(dir);
                counts.passed = counts.tests - counts.failed - counts.errors - counts.skipped;
              } catch (e) { /* artifacts may not exist yet */ }
              return counts;
            }

            // --- Parse lcov for coverage percentage (AC3, AC9) ---
            function parseCoverage(dir) {
              try {
                const walk = (d) => {
                  for (const entry of fs.readdirSync(d, { withFileTypes: true })) {
                    const full = path.join(d, entry.name);
                    if (entry.isDirectory()) { const r = walk(full); if (r) return r; }
                    else if (entry.name === 'lcov.info') return full;
                  }
                  return null;
                };
                const lcov = fs.existsSync(dir) ? walk(dir) : null;
                if (!lcov) return null;
                const content = fs.readFileSync(lcov, 'utf8');
                let totalLines = 0, coveredLines = 0;
                for (const line of content.split('\n')) {
                  if (line.startsWith('LF:')) totalLines += parseInt(line.slice(3));
                  if (line.startsWith('LH:')) coveredLines += parseInt(line.slice(3));
                }
                return totalLines > 0 ? ((coveredLines / totalLines) * 100).toFixed(1) : null;
              } catch (e) { return null; }
            }

            // --- Extract failure messages from JUnit XML (AC4) ---
            function extractFailures(dir) {
              const failures = [];
              try {
                const walk = (d) => {
                  for (const entry of fs.readdirSync(d, { withFileTypes: true })) {
                    const full = path.join(d, entry.name);
                    if (entry.isDirectory()) walk(full);
                    else if (entry.name.endsWith('.xml')) {
                      const xml = fs.readFileSync(full, 'utf8');
                      const matches = xml.match(/<failure[^>]*>[\s\S]*?<\/failure>/g) || [];
                      for (const m of matches.slice(0, 5)) {
                        const msg = m.replace(/<\/?failure[^>]*>/g, '').trim();
                        if (msg) failures.push(msg.substring(0, 500));
                      }
                    }
                  }
                };
                if (fs.existsSync(dir)) walk(dir);
              } catch (e) { /* ignore */ }
              return failures;
            }

            // --- Gather data ---
            const unitCounts = parseJUnit('all-artifacts/unit-test-results-node-20');
            const integrationCounts = parseJUnit('all-artifacts/integration-test-results');
            const e2eCounts = parseJUnit('all-artifacts/playwright-report');
            const coverage = parseCoverage('all-artifacts/unit-test-results-node-20');

            const fmtTime = (s) => s > 0 ? `${Math.floor(s / 60)}m ${Math.round(s % 60)}s` : '-';

            // --- Build PR comment (AC3, AC6) ---
            const rows = [
              { name: 'Unit Tests (Node 18 + 20)', status: jobResults.unit, c: unitCounts },
              { name: 'Integration Tests', status: jobResults.integration, c: integrationCounts },
              { name: 'E2E Tests (Critical)', status: jobResults.e2e, c: e2eCounts },
            ];

            let body = '## Test Results Summary\n\n';
            body += '| Suite | Status | Passed | Failed | Skipped | Time |\n';
            body += '|-------|--------|--------|--------|---------|------|\n';
            for (const r of rows) {
              const has = r.c.tests > 0;
              body += `| ${r.name} | ${statusIcon(r.status)} ${r.status}`;
              body += ` | ${has ? r.c.passed : '-'}`;
              body += ` | ${has ? r.c.failed + r.c.errors : '-'}`;
              body += ` | ${has ? r.c.skipped : '-'}`;
              body += ` | ${fmtTime(r.c.time)} |\n`;
            }

            body += '\n';
            if (coverage) {
              const icon = parseFloat(coverage) >= 80 ? '✅' : '⚠️';
              body += `**Coverage:** ${icon} ${coverage}% (threshold: 80%)\n\n`;
            }

            // --- Failed test details with stack traces (AC4) ---
            if (!allPassed) {
              const allFailures = [
                ...extractFailures('all-artifacts/unit-test-results-node-20'),
                ...extractFailures('all-artifacts/integration-test-results'),
                ...extractFailures('all-artifacts/playwright-report'),
              ].slice(0, 10);

              if (allFailures.length > 0) {
                body += '### Failed Test Details\n\n';
                body += '<details>\n<summary>Stack traces and error logs</summary>\n\n';
                for (const f of allFailures) {
                  body += '```\n' + f + '\n```\n\n';
                }
                body += '</details>\n\n';
              }
            }

            body += allPassed
              ? '**✅ All checks passed!**'
              : '**❌ Some checks failed.** Review the failing jobs above for details and stack traces.';
            body += '\n\n---\n';
            body += `*Coverage report available on [Codecov](https://codecov.io/gh/${{ github.repository }})*\n`;
            body += `*Allure report: [reports.qualisys.io](https://reports.qualisys.io/allure-docker-service/projects/qualisys/reports/latest)*\n`;
            body += `*Run: [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})*`;

            // --- Upsert PR comment (avoid spam) ---
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('## Test Results Summary')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body,
              });
            }

  # ===================================================================
  # Publish Allure Report (Story 0-17, AC7)
  # Downloads Allure results from all test jobs, merges, and uploads
  # to Allure Server at reports.qualisys.io
  # ===================================================================
  publish-allure-report:
    name: Publish Allure Report
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-artifacts
        continue-on-error: true

      - name: Merge Allure results
        run: |
          mkdir -p merged-allure-results
          find all-artifacts -path "*/allure-results/*" -type f \
            -exec cp {} merged-allure-results/ \; 2>/dev/null || true
          echo "Merged $(ls merged-allure-results | wc -l) Allure result files"

      - name: Upload to Allure Server
        if: hashFiles('merged-allure-results/*') != ''
        run: |
          cd merged-allure-results
          FILES=$(find . -type f -name "*.json" -o -name "*.txt" -o -name "*.png" -o -name "*.xml" | head -500)
          if [ -n "$FILES" ]; then
            tar czf /tmp/allure-results.tar.gz .
            curl -sf -X POST \
              -H "Content-Type: multipart/form-data" \
              -F "results=@/tmp/allure-results.tar.gz" \
              "${{ secrets.ALLURE_SERVER_URL }}/allure-docker-service/send-results?project_id=qualisys" || \
              echo "::warning::Allure Server upload failed (server may not be deployed yet)"
          fi

      - name: Generate Allure Report
        if: hashFiles('merged-allure-results/*') != ''
        run: |
          curl -sf -X GET \
            "${{ secrets.ALLURE_SERVER_URL }}/allure-docker-service/generate-report?project_id=qualisys" || \
            echo "::warning::Allure report generation failed (server may not be deployed yet)"
